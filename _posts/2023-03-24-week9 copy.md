---
layout: post
title: Week 9
---

This  week, I tested some different loss functions with our model. We're using a categorical cross-entropy loss function because our targets are "one-hot encoded," which means that the categories into which our model is classifying messages are encoded as binary vectors so the model can make predictions about them. Some of the distinguishing features of loss functions are intelligible to me, like the binary cross-entropy function and sparse categorical cross-entropy function, while others are more difficult for me to grasp, like the Kullback_Leibler Divergence. I used dummy data similar to what we'll expect to see if any alternate functions would be more suitable than what we are currently using. Nothing was obviously better, and I ran into several errors.

On the interface side, I've been updating the code to work with a MongoDB database of users with associated data. I'm new to the complexity of connecting reducers to thunks to controllers to models to schemas etc, and the number of stops necessary for some data to go between server and application is difficult to keep track of.

