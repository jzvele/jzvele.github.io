---
layout: post
title: Week 4
---

We're slowly building a scoreboard of different models and their accuracy. The bag of words approach, unsurprisingly, has been the least successful with an F1 score of 0.46. The best performances so far have been using Bert and DistilBert, with F1 scores of 0.83 and 0.86 respectively. 

One change we have made to the data is breaking down messages with multiple clauses into separate messages to be analyzed. This is syntactic parsing at the most basic level, but it is interesting that we are already blending classical AI techniques with machine learning. I'm a little bit biased towards classical techniques because they are easier to understand from a linguistics perspective rather than a statistics perspective, but some hardcoded structure is clearly helping our model. Nidutt also made a version of the model that masks dimensional target words like Gallery or Tuesday to test whether those words are creating some kind of unhelpful noise, and it did improve the scores.

Some of our classification data coding is still inconsistent, so we will have to go through it again to ensure that sentences with words like "I think" or "it might be" or "likely" are uniformly coded as maybes. Same with unlikely and not likely. 
