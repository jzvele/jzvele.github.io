---
layout: post
title: Week 3
---

This week, we have a working model based on a simple bag-of-words approach. It is not wholly reliable and does not classify maybe-yes or maybe-no very often, but it is something to improve on. 

I've created a few versions of the interface. The most intuitive setup to me is a split screen with two chat windows--one for communicating with human teammates and one for receiving messages from the AI "teammate." I made some wireframes with other styles of intervention, like displaying a chart that shows each team member's opinion about the various options based on the agent's theory-of-mind model, or icons representing each teammate that look green, yellow, or red depending on the agent's perception of how well the user has communicated with them. The benefit of the chat option is that it is consistent with the framing of this agent as a "teammate" and "collaborator." The other options make it seem much more like a tool. It would be interesting to test the various styles of intervention to determine which is most effective.

Along those lines, I'm going to make some prototypes that give the agent different voices--declarative vs deferential, active vs passive, formal vs friendly. A study from 2020 found that people were more communicative and equanimous when paired with a robot with a vulnerable voice. The practical utility of our AI teammate could be perceived very differently depending on its presentation.

